{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of decision_trees_in_class_short_illustration_splitting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuWqDRAcJO+DVwRo2XdGZu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramonVDAKKER/teaching/blob/main/actuarial_institute_crash_course_some_snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8khFW1eoeAFH"
      },
      "source": [
        "# The basics of decision trees\n",
        "\n",
        "This notebook contains snippets to facilitate a demo and discussion during a lecture. We focus on the technique and not on understanding the empirical application associated to the dataset we use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrVdEnVveDjn"
      },
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CJCn2cadNeF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import plot_partial_dependence\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RV02kMLhRne"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U pandas-profiling[notebook]\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "import pandas_profiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlbhxSw0ealm"
      },
      "source": [
        "## 1. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_4q5HhSd7oX"
      },
      "source": [
        "data = load_breast_cancer()\n",
        "y_df = pd.DataFrame(data.target, columns=[\"target\"])\n",
        "X_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.3, random_state=123)\n",
        "data_train_df = pd.concat([y_train_df, X_train_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjZ8dXCAd7uL"
      },
      "source": [
        "print(f\"The dataset, available for training, has {len(data_train_df)} rows and {data_train_df.shape[1]} columns.\")\n",
        "print(\"Some rows of the dataset:\")\n",
        "display(data_train_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlf3ZdKgZmJ"
      },
      "source": [
        "## 2. Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fSLyD6d5GC"
      },
      "source": [
        "profile = pandas_profiling.ProfileReport(data_train_df, title=\"Data report\", html={\"style\": {\"full_width\": True}}, minimal=True)\n",
        "profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsEMo2g-o5pT"
      },
      "source": [
        "## 3. Splitting mechanism illustrated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOQyNULsp6z-"
      },
      "source": [
        "def gini_impurity(y_left, y_right):\n",
        "  p_L = np.mean(y_left.values)\n",
        "  gini_left = 2 * p_L * (1 - p_L)\n",
        "  p_R = np.mean(y_right.values)\n",
        "  gini_right = 2 * p_R * (1 - p_R)\n",
        "  n_right = len(y_right)\n",
        "  weight_right = n_right / (len(y_left) + n_right)\n",
        "  gini = weight_right * gini_right + (1 - weight_right) * gini_left\n",
        "  return gini, gini_left, gini_right\n",
        "def determine_gini_impurity_for_feature_and_split_ext(y, X_df, name_feature, threshold):\n",
        "  left = (X_df[name_feature] <= threshold)\n",
        "  y_left = y[left]\n",
        "  y_right = y[~ left]\n",
        "  return gini_impurity(y_left, y_right) \n",
        "def determine_gini_impurity_for_feature_and_split(y, X_df, name_feature, threshold):\n",
        "  gini, _, _ = determine_gini_impurity_for_feature_and_split_ext(y, X_df, name_feature, threshold)\n",
        "  return gini\n",
        "vector_determine_gini_impurity_for_feature_and_split = np.vectorize(determine_gini_impurity_for_feature_and_split, excluded=[\"y\", \"X_df\"])\n",
        "def analyze_feature(X_df, name_feature, y):\n",
        "  feature_values = X_df[name_feature].sort_values(ascending=True)[1:-1]\n",
        "  gini = vector_determine_gini_impurity_for_feature_and_split(y=y, X_df=X_df, name_feature=name_feature, threshold=feature_values)\n",
        "  plt.plot(feature_values, gini)\n",
        "  plt.ylabel(\"Gini\")\n",
        "  plt.xlabel(\"Splitvalue\")\n",
        "  plt.title(f\"Gini as function of possible split values for feature {name_feature}.\")\n",
        "  print(f\"Split value: {feature_values.iloc[gini.argmin()]}\")\n",
        "  print(f\"Gini: {gini.min()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPHybiL0Dhjq"
      },
      "source": [
        "Let us analyze a feature and evaluate how we should split for this feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUwfRrytEA7S"
      },
      "source": [
        "name_feature = \"mean perimeter\"\n",
        "sns.histplot(data=data_train_df, x=name_feature, hue=\"target\", multiple=\"stack\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5r9N6fF-SM"
      },
      "source": [
        "The following graph shows how the Gini varies with the threshold:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j4a1yZMrUP3"
      },
      "source": [
        "analyze_feature(X_train_df, name_feature, y_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6bov-souq8f"
      },
      "source": [
        "def determine_best_split(y, X_df):\n",
        "  out_df = pd.DataFrame(columns=[\"Gini\", \"Split\"])\n",
        "  for name_feature in X_df.columns:\n",
        "    feature_values = X_df[name_feature].sort_values(ascending=True)[1:-1]\n",
        "    gini = vector_determine_gini_impurity_for_feature_and_split(y=y, X_df=X_df, name_feature=name_feature, threshold=feature_values)\n",
        "    out_df.loc[name_feature, \"Split\"] = feature_values.iloc[gini.argmin()]\n",
        "    out_df.loc[name_feature, \"Gini\"] = gini.min()\n",
        "  return out_df\n",
        "display(determine_best_split(y_train_df, X_train_df).sort_values(by=\"Gini\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov0s0LUzzj-t"
      },
      "source": [
        "## 4. Train decision tree using Scikit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlwQk0JIG7ck"
      },
      "source": [
        "Let us check that Scikit finds the same answer for the first split. To this end we train a decision tree with 1 as maximum depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V38OrS5wxcFp"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier(max_depth=1)\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "tree.plot_tree(dtree) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Wj_Np93GIm"
      },
      "source": [
        "name_feature = X_train_df.columns[20]\n",
        "sns.histplot(data=data_train_df, x=name_feature, hue=\"target\", multiple=\"stack\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_5Sh5pUHdUz"
      },
      "source": [
        "Let us check if our ad hoc code yields the same Gini for both leaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvPb0bG99Jis"
      },
      "source": [
        "print(f\"Gini before splitting:  {(2 * np.mean(y_train_df.values) * (1 - np.mean(y_train_df.values))):.03f}\")\n",
        "split_value = 16.795\n",
        "gini, gini_left, gini_right = determine_gini_impurity_for_feature_and_split_ext(y_train_df, X_train_df, name_feature, split_value)\n",
        "print(f\"Gini left: {gini_left:.03f}.\")\n",
        "print(f\"Gini right: {gini_right:.03f}.\")\n",
        "frac_left = np.mean(X_train_df[name_feature] <= split_value)\n",
        "print(f\"Gini after splitting: {frac_left * gini_left + (1 - frac_left) * gini_right:.03f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHUzcgLvIZnM"
      },
      "source": [
        "Now we understand how the decision tree comes to its splits, we train a \"full\" tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FvsDSe_rwS"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "tree.plot_tree(dtree) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZxS9YnjBWE-"
      },
      "source": [
        "Use the tree to predict $Y$ for an observation $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkx5eJKKBmpH"
      },
      "source": [
        "x = X_train_df.loc[0]\n",
        "print(\"For\\n\\n\")\n",
        "print(x)\n",
        "print(f\"\\n\\nthe tree yields Y_hat={dtree.predict(x.values.reshape(1, -1))[0]}.\")\n",
        "print(f\"The true outcome is {y_train_df.loc[0][0]}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2J1_UHBDJb4"
      },
      "source": [
        "Now we \"score\" full dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrnkOaSjAPJi"
      },
      "source": [
        "y_train_pred = dtree.predict(X_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGrTKHteAYno"
      },
      "source": [
        "print(y_train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezQj96hgDaVp"
      },
      "source": [
        "Let us determine the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWA9zwMhAadG"
      },
      "source": [
        "class_names = [0, 1]\n",
        "def confusion_matrix(model_object, X_df, y_df):\n",
        "    disp = plot_confusion_matrix(model_object, X_df, y_df)\n",
        "    disp.ax_.set_title(\"Confusion matrix\")\n",
        "    plt.show()\n",
        "    y_hat = model_object.predict(X_df)\n",
        "    accuracy = accuracy_score(y_df, y_hat, normalize=True)\n",
        "    print(f\"The accuracy is {100 * accuracy:0.2f}%.\")\n",
        "confusion_matrix(dtree, X_train_df, y_train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8uQG8y7DlCw"
      },
      "source": [
        "Next we evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7YW21oAagO"
      },
      "source": [
        "confusion_matrix(dtree, X_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRhnksiE59z"
      },
      "source": [
        "Let us also fit a decision tree with restricted depth and evaluate its performance on the train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrNAAL9REn3D"
      },
      "source": [
        "dtree = tree.DecisionTreeClassifier(max_depth=3)\n",
        "dtree = dtree.fit(X_train_df, y_train_df)\n",
        "confusion_matrix(dtree, X_train_df, y_train_df)\n",
        "confusion_matrix(dtree, X_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXPlXniBDy9s"
      },
      "source": [
        "## 5. Random Forest using Scitkit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575wHJhqDyec"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf = rf.fit(X_train_df, y_train_df.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k94BgTAsaDLU"
      },
      "source": [
        "The random forest objects contains 100 trees. Let us have a look and calculate predictions, for a given observation, for all trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYeNM4FoYxJ-"
      },
      "source": [
        "x = X_train_df.loc[0].values.reshape(1, -1)\n",
        "for t in range(0, 100):\n",
        "  print(f\"Prediction of tree {t}: Y_hat={ rf.estimators_[t].predict(x)[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0cYa4HaAQS"
      },
      "source": [
        "You typically do not use this information, but rely directly on the predict-method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLB9v4EPZ_0t"
      },
      "source": [
        "rf.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HToP-JmcaeVY"
      },
      "source": [
        "Let us evaluate the performance of the random forest on the train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GavK8hSWakPJ"
      },
      "source": [
        "confusion_matrix(rf, X_train_df, y_train_df)\n",
        "confusion_matrix(rf, X_test_df, y_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m96UjoBwcMGN"
      },
      "source": [
        "As we have 100 decision trees, it is difficult to say how the features impact the predictions. Two standard tools are global feature importances and partial dependence plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-uvJeDcOG_"
      },
      "source": [
        "importances = rf.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
        "forest_importances = pd.Series(importances, index=X_train_df.columns)\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llbOs7WudzlW"
      },
      "source": [
        "plot_partial_dependence(rf, X_train_df, features = [\"worst radius\", \"worst area\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHmy2blCfbjx"
      },
      "source": [
        "## 6. Fitting a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTvn1sW7e1B7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUSLb7pzlA6x"
      },
      "source": [
        "# initialize NN\n",
        "nn = Sequential()\n",
        "# add first layer with xxx nodes and using ReLu as activation function\n",
        "nn.add(Dense(10, activation=\"relu\", input_shape=(X_train_df.shape[1],), kernel_initializer=\"uniform\"))\n",
        "# add second layer with xxx nodes and using ReLu as activation function\n",
        "nn.add(Dense(10, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
        "# add output layer (binary target)\n",
        "nn.add(Dense(1, activation=\"sigmoid\")) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMRQYLgdlO3X"
      },
      "source": [
        "# Compile the model\n",
        "nn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# Verify that model contains information from compiling\n",
        "print(f\"Loss function: {nn.loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qEzgxcslPF8"
      },
      "source": [
        "# Fit the model\n",
        "nn.fit(X_train_df, y_train_df,  epochs=200, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0x9sb_qlkfs"
      },
      "source": [
        "y_hat_train_nn = nn.predict(X_train_df)\n",
        "print(y_hat_train_nn[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwevq8wrlkiG"
      },
      "source": [
        "y_hat_train_nn = (y_hat_train_nn > .5)\n",
        "accuracy = accuracy_score(y_train_df, y_hat_train_nn, normalize=True)\n",
        "print(f\"The accuracy is {100 * accuracy:0.2f}%.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WVklQbMlkl6"
      },
      "source": [
        "y_hat_test_nn = nn.predict(X_test_df)\n",
        "y_hat_test_nn = (y_hat_test_nn > .5)\n",
        "accuracy = accuracy_score(y_test_df, y_hat_test_nn, normalize=True)\n",
        "print(f\"The accuracy is {100 * accuracy:0.2f}%.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}